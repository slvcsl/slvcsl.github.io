<!DOCTYPE html>
<html lang='en-us' ><meta charset="utf-8">
<meta name="viewport" content="width=device-width">


<title>Silvia Casola</title>

<meta name="generator" content="Hugo Eureka 0.8.4" />
<link rel="stylesheet" href="https://slvcsl.github.io/css/eureka.min.css">
<script defer src="https://slvcsl.github.io/js/eureka.min.js"></script>

<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/styles/solarized-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.1.0/build/highlight.min.js"
   crossorigin></script>

<script defer src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/js/all.min.js"
   integrity="sha256-uNYoXefWRqv&#43;PsIF/OflNmwtKM4lStn9yrz2gVl6ymo="  crossorigin></script>









<meta name="description"
  content="">



<meta property="og:title" content="Silvia Casola" />
<meta property="og:type" content="website" />



<meta property="og:url" content="https://slvcsl.github.io/" />









<meta property="og:locale" content="en-us" />




<meta property="og:site_name" content="Silvia Casola" />






<meta property="og:updated_time" content="2020-10-16T00:00:00&#43;00:00" />



<meta property="article:section" content="" />


<link rel="alternate" type="application/rss+xml" href="https://slvcsl.github.io/index.xml" title="Silvia Casola" />

<body class="flex flex-col min-h-screen">
  <header class="fixed flex items-center w-full min-h-16 pl-scrollbar z-50 bg-secondary-bg shadow-sm">
    <div class="w-full max-w-screen-xl mx-auto"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="./" class="mr-6 text-primary-text text-xl font-bold">Silvia Casola</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:flex-grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:flex-grow pb-4 md:pb-0 border-b md:border-b-0">
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col left-0 md:left-auto right-auto md:right-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">Light</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">Dark</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">Auto</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
  </header>
  <main class="flex-grow pt-16">
  
  
  
    
      
  

  

  
  
  

  
    
    
    
    
      
      
        
      
    
  

  
  
  
  
    
    
    
    
      
      
    

    
    
    
    
    
  

  <div class="pl-scrollbar bg-secondary-bg" 
    >
    <div class="max-w-screen-xl mx-auto">
      <div id="about" class="lg:w-3/4 mx-auto px-6 md:px-8 xl:px-12 py-12">
        
  

  
    <div class="flex flex-col md:flex-row items-center justify-center mb-12">
  
  
    <div class="flex-none w-48 mx-auto md:ml-0 md:mr-8 md:pr-8 md:border-r">
      <img src="https://slvcsl.github.io/profile.jpeg" class="rounded-full" alt="Avatar">
    </div>
  
  <div class="flex-grow mt-4 md:mt-0">
    <div class="text-3xl py-4">Silvia Casola</div>
    <div class="w-3/12 xl:w-2/12 border-b"></div>

    <div class="flex items-center pt-4">
      
        <i class="fas fa-user"></i>
      
      <div class="flex flex-wrap">
        
          <span class="pl-4">Ph.D student</span>
        

        
          <a href="https://ict.fbk.eu/units/nlp/" class="pl-4">University of Padua; Fondazione Bruno Kessler</a>
        
      </div>

    </div>

    
      <div class="py-8 text-lg leading-normal">
        My updated CV is <strong><a href="Academic_cv.pdf">here</a></strong>
      </div>
    
  </div>
  <div class="flex md:flex-col justify-center items-end ml-8">
    
    
      
      
      
      
      
      <div class="pb-2 pr-4 md:pr-0 pt-4 md:pt-0">
        <a href="mailto:scasola@fbk.eu"><i class="fas fa-envelope"></i></a>
      </div>
    
      
      
      
      
      
      <div class="pb-2 pr-4 md:pr-0 pt-4 md:pt-0">
        <a href="https://www.linkedin.com/in/silvia-casola/"><i class="fab fa-linkedin"></i></a>
      </div>
    
      
      
      
      
      
      <div class="pb-2 pr-4 md:pr-0 pt-4 md:pt-0">
        <a href="https://github.com/slvcsl"><i class="fab fa-github"></i></a>
      </div>
    
      
      
      
      
      
      <div class="pb-2 pr-4 md:pr-0 pt-4 md:pt-0">
        <a href="https://twitter.com/eclipto93"><i class="fab fa-twitter"></i></a>
      </div>
    
  </div>
</div>
<div class="content">
  <p>I am a final-year PhD student at the University of Padua (Brain, Mind and Computer Science program), with a grant from Fondazione Bruno Kessler. My advisor is Alberto Lavelli. I own a Master&rsquo;s degree cum laude in Computer Engineering (Data Science curriculum) from Politecnico di Torino.</p>
<p>I visited the TALN group in UPF (Barcelona) to work on summarization and simplification under the supervision of Prof. Saggion and interned at Huawei Research Ireland for 6 months.</p>
<p>My research interests include automatic text summarization and simplification (with a focus on technical text), Natural Language Generation, Natural Language Processing and Deep Learning.</p>

</div>

  

      </div>
    </div>
  </div>

    

  
  
    
      
  

  

  
  
  

  
    
    
    
    
      
      
    
  

  
  
  
  
    
    
    
    
      
      
        
      
    

    
    
    
    
    
  

  <div class="pl-scrollbar bg-primary-bg" 
    >
    <div class="max-w-screen-xl mx-auto">
      <div id="experiences" class="lg:w-4/5 mx-auto px-6 md:px-8 xl:px-12 py-12">
        
  

  

    
    
    
    
    
      
    
    <div class="flex flex-col lg:flex-row">
      <div class="flex-none lg:w-1/4 lg:mr-4">
        <h2 class="font-bold text-3xl my-4">Publications</h2>
      </div>
      <div class="flex-grow lg:ml-4">
        
<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">What&#39;s in a (dataset&#39;s) name? The case of BigPatent</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>Workshop on Generation, Evaluation &amp; Metrics (GEM) at EMNLP       </div>
		</span> <div align="right" class="flex-shrink-0"> 2022</div>
		
    </div><br>

    <div class="content">
      Sharing datasets and benchmarks has been crucial for rapidly improving Natural Language Processing models and systems. Documenting datasets' characteristics (and any modification introduced over time) is equally important to avoid confusion and make comparisons reliable. Here, we describe the case of BigPatent, a dataset for patent summarization that exists in at least two rather different versions under the same name. While previous literature has not clearly distinguished among versions, their differences not only lay on a surface level but also modify the dataset&rsquo;s core nature and, thus, the complexity of the summarization task.  While this paper describes a specific case, we aim to shed light on new challenges that might emerge in resource sharing and advocate for comprehensive documentation of datasets and models.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <span></span>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">Exploring the limits of a base BART for multi-document summarization in the medical domain</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>Proceedings of the Third Workshop on Scholarly Document Processing       </div>
		</span> <div align="right" class="flex-shrink-0"> 2022</div>
		
    </div><br>

    <div class="content">
      This paper describes our participation in the Multi-document Summarization for Literature Review (MSLR) Shared Task, to explore summarization for automatically reviewing scientific results. Rather than aiming at maximizing the metrics using expensive computational models, we placed ourselves in a situation of scarce computational resources and explore the limits of a base sequence to sequence models (thus with a limited input length) to the task. We explored methods to feed the abstractive model with salient sentences only (using a first extractive step); we found some improvements, but results tend to be inconsistent.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="https://aclanthology.org/2022.sdp-1.23/">Paper</a>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">Summarization, Simplification, and Generation: The Case of Patents</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>Expert Systems with Applications       </div>
		</span> <div align="right" class="flex-shrink-0"> 2022</div>
		
    </div><br>

    <div class="content">
      We survey Natural Language Processing (NLP) approaches to summarizing, simplifying, and generating patents' text. While solving these tasks has important practical applications - given patents' centrality in the R&amp;D process - patents' idiosyncrasies open peculiar challenges to the current NLP state of the art. This survey aims at a) describing patents' characteristics and the questions they raise to the current NLP systems, b) critically presenting previous work and its evolution, and c) drawing attention to directions of research in which further work is needed. To the best of our knowledge, this is the first survey of generative approaches in the patent domain.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417422009356?via%3Dihub">Paper</a>
          

          
          <a href="https://arxiv.org/abs/2104.14860">arxiv</a>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">Pre-trained transformers: an empirical comparison</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>Machine Learning with Applications       </div>
		</span> <div align="right" class="flex-shrink-0"> 2022</div>
		
    </div><br>

    <div class="content">
      Pre-trained transformers have rapidly become very popular in the Natural Language Processing (NLP) community, surpassing the previous state of the art in a wide variety of tasks. While their effectiveness is indisputable, these methods are expensive to fine-tune on the target domain due to the high number of hyper-parameters; this aspect significantly affects the model selection phase and the reliability of the experimental assessment. This paper serves a double purpose: we first describe five popular transformer models and survey their typical use in previous literature, focusing on reproducibility; then, we perform comparisons in a controlled environment over a wide range of NLP tasks. Our analysis reveals that only a minority of recent NLP papers that use pre-trained transformers reported multiple runs (20%), standard deviation or statistical significance (10%), and other crucial information, seriously hurting replicability and reproducibility. Through a vast empirical comparison on real-world datasets and benchmarks, we also show how the hyper-parameters and the initial seed impact results, and highlight the low models’ robustness.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="https://www.sciencedirect.com/science/article/pii/S2666827022000445">Paper</a>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">WITS: Wikipedia for Italian Text Summarization</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>CLIC-IT       </div>
		</span> <div align="right" class="flex-shrink-0"> 2021</div>
		
    </div><br>

    <div class="content">
      Abstractive text summarization has recently improved its performance due to the use of sequence to sequence models. However, while these models are extremely data-hungry, datasets in languages other than English are few. In this work, we introduce WITS (Wikipedia for Italian Text Summarization), a largescale dataset built exploiting Wikipedia articles’ structure. WITS contains almost 700,000 Wikipedia articles, together with their human-written summaries. Compared to existing data for text summarization in Italian, WITS is more than an order of magnitude larger and more challenging given its lengthy sources. We explore WITS characteristics and present some baselines for future work.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="http://ceur-ws.org/Vol-3033/paper65.pdf">Paper</a>
          

          
          <span></span>
          


		  
		  
		  ~
          <a href="https://drive.google.com/file/d/1W-52p4_oUakbr57zZvadoRKSP-vndGlJ/view?usp=sharing">Dataset</a>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">Investigating Continued pretraining for Zero-Shot Cross-Lingual SpokenLanguage Understanding.</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>CLIC-IT       </div>
		</span> <div align="right" class="flex-shrink-0"> 2021</div>
		
    </div><br>

    <div class="content">
      Spoken Language Understanding (SLU) in task-oriented dialogue systems involves both intent classification (IC) and slot filling (SF) tasks. The de facto method for zero-shot cross-lingual SLU consists of fine-tuning a pretrained multilingual model on English labeled data before evaluating the model on unseen languages. However, recent studies show that adding a second pretraining stage (continued pretraining) can improve performance in certain settings. This paper investigates the effectiveness of continued pretraining on unlabeled spoken language data for zero-shot cross-lingual SLU. We demonstrate that this relatively simple approach benefits either SF and IC task across 8 target languages, especially the ones written in Latin script. We also find that discrepancy between languages used during pretraining and fine-tuning may introduce training instability, which can be alleviated through code-switching.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="http://ceur-ws.org/Vol-3033/paper43.pdf">Paper</a>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl"> FBK@SMM4H2020: RoBERTa for detecting medications on Twitter</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>#SMM4H@Coling       </div>
		</span> <div align="right" class="flex-shrink-0"> 2020</div>
		
    </div><br>

    <div class="content">
      This paper describes a classifier for tweets that mention medications or supplements, based on a pretrained transformer. We developed such a system for our participation in Subtask 1 of the Social Media Mining for Health Application workshop, which featured an extremely unbalanced dataset. The model showed promising results, with an F1 of 0.8 (task mean: 0.66).
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="https://aclanthology.org/2020.smm4h-1.15.pdf">Paper</a>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

<div class="mb-6">
  <div class="bg-secondary-bg rounded border hover:shadow-lg transition ease-in-out duration-200 px-6 pt-6 pb-4">
    <div class="mb-4">
      <div class="font-bold text-xl">Mental Workload Assessment for UAV Traffic Control Using Consumer-Grade BCI Equipment</div>
	  <div></div><br>
      <div class="flex flex-col md:flex-row md:justify-between">
        <div>
		<span>IHCI       </div>
		</span> <div align="right" class="flex-shrink-0"> 2017</div>
		
    </div><br>

    <div class="content">
      The increasing popularity of unmanned aerial vehicles (UAVs) in critical applications makes supervisory systems based on thepresence of human in the control loop of crucial importance. In UAV-traffic monitoring scenarios, where human operators are responsible formanaging drones, systems flexibly supporting different levels of autonomy are needed to assist them when critical conditions occur. The assessment of UAV controllers’ performance thus their mental workload maybe used to discriminate the level and type of automation required. The aim of this paper is to build a mental workload prediction model based onUAV operators’ cognitive demand to support the design of an adjustable autonomy supervisory system. A classification and validation procedure was performed to both categorize the cognitive workload measured by ElectroEncephaloGram signals and evaluate the obtained patterns from the point of view of accuracy. Then, a user study was carried out to identify critical workload conditions by evaluating operators’ performance inaccomplishing the assigned tasks. Results obtained in this study provided precious indications for guiding next developments in the field.
    </div>
	<span style="color:darkolivegreen;font-weight:bold">
	          
          <span></span>
          

          

          
          <a href="https://rdcu.be/cFyhP">Paper</a>
          

          
          <span></span>
          


		  
		  
          <span></span>
          
          
        </div>
  </div>
</div>

      </div>
    </div>
  

      </div>
    </div>
  </div>

    

  
  
    
      
  

  

  
  
  

  
    
    
    
    
      
      
    
  

  
  
  
  
    
    
    
    
      
      
    

    
    
    
    
    
  

  <div class="pl-scrollbar bg-primary-bg" 
    >
    <div class="max-w-screen-xl mx-auto">
      <div id="blank" class="lg:w-3/4 mx-auto px-6 md:px-8 xl:px-12 py-12">
        
  

  
    
<div style="text-align:center">
  <p></p>
</div>
  

      </div>
    </div>
  </div>

    

  
  
  </main>
  <footer class="pl-scrollbar">
    <div class="w-full max-w-screen-xl mx-auto"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text"> Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
  </footer>
</body>

</html>